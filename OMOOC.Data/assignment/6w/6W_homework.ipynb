{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定一个函数 f(x) = x^2 + 3*x - 10 , 完成以下题目：\n",
    "\n",
    "理解方程求根中的二分法(Bisection)，并使用基本的numpy库而非scipy库，来实现算法。  \n",
    "使用上述算法来得到函数f(x)的根。  \n",
    "理解梯度下降法(Gradient descent)，并使用基本的numpy库而非scipy库，来实现算法。  \n",
    "使用上述算法来得到函数f(x)的最小值。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 函数初探"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2 + 3*x - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=np.linspace(-10,10,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAssAAAEACAYAAABFzzsxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmqx/8Pew3bx3/fcp4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff066730850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,4))\n",
    "\n",
    "ax.plot(x,f(x))\n",
    "ax.set_xticks(np.linspace(-10,10,21))\n",
    "ax.axhline(0, color='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bisection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.什么是二分法\n",
    "\n",
    "对于区间[a,b]上连续不断且 f(a)\\*f(b)<0 的函数y=f(x)，通不过不断地把函数f(x)的零点所在的区间一分为二，使区间的两个端点逐步逼近零点，进而得到零点近似值的方法叫二分法。  \n",
    "我们可以发现二分法的几个特点：  \n",
    "（1）f(x)在区间内连续不断;  \n",
    "（2）零点在区间之间;  \n",
    "（3）逐步迭代逼近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.二分法的步骤\n",
    "\n",
    "2.1 确定区间[a,b]，验证f(a)\\*f(b)<0，给定精确度ε;  \n",
    "2.2 求区间(a,b)的中点c;  \n",
    "2.3 计算f(c)：\n",
    "（1）若f(c)=0,则c就是函数的零点；  \n",
    "（2）若f(a)\\*f(c)<0，则令b=c（此时，零点在（a,c）之间）;  \n",
    "（3）若f(b)\\*f(c)<0，则令a=c（此时，零点在（c,b）之间）;  \n",
    "2.4 判断是否达到精确度ε;若|a-b|<ε，则得到零点近似值a（或b）;  \n",
    "否则重复2.3的（2）和（3）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.二分法的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 标准Python实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 标准python\n",
    "\n",
    "def bisection1(f,a,b):\n",
    "    eps = 1e-5\n",
    "    c = (a+b)/2.0\n",
    "    \n",
    "    if f(a)*f(b)*f(c) == 0:\n",
    "        if f(a) == 0:\n",
    "            x0 = a\n",
    "        elif f(b) == 0:\n",
    "            x0 = b\n",
    "        else:\n",
    "            x0 =c\n",
    "    else:\n",
    "        while f(a)*f(b)<0:\n",
    "            if f(a)*f(c)<0:\n",
    "                b = c\n",
    "            elif f(b)*f(c)<0:\n",
    "                a = c\n",
    "            \n",
    "            elif abs(a-b) < eps:\n",
    "                x0 = a\n",
    "    return x0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bisection1(f,-6,-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bisection1(f,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 标准python2\n",
    "\n",
    "def bisection2(a,b,eps):\n",
    "    c = (a+b)/2.0\n",
    "    while (b-a)/2.0 >eps:\n",
    "        if f(c) == 0:\n",
    "            return c\n",
    "        elif f(a)*f(c) < 0:\n",
    "            b = c\n",
    "        else:\n",
    "            a = c\n",
    "        c = (a+b)/2.0\n",
    "        \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bisection2(-6,-4,1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bisection2(1,3,1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 16.90 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000000 loops, best of 3: 1.24 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "bisection1(f,-6,-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 31.18 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000000 loops, best of 3: 550 ns per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "bisection2(-6,-4,1e-5)\n",
    "\n",
    "# 算法2更优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Numpy实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bisectionNumpy1(a,b,eps):\n",
    "    c = (a+b)/2.0\n",
    "    while (b-a)/2.0 >eps:\n",
    "        if f(c) == 0:\n",
    "            return c\n",
    "        else:\n",
    "            b = np.where(f(a)*f(c)<0,c,b)\n",
    "            a = np.where(f(b)*f(c)<0,c,a)\n",
    "#        c = (a+b)/2.0\n",
    "#    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.0"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bisectionNumpy1(-6,-4,1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bisectionNumpy1(1,3,1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 12.03 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000000 loops, best of 3: 575 ns per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "bisectionNumpy1(-6,-4,1e-5)\n",
    "\n",
    "# 速度略快于标准python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bisectionNumpy2(a,b,eps):\n",
    "    c = (a+b)/2.0\n",
    "    while ((b-a)/2.0 >eps)&(f(c) != 0):\n",
    "        b = np.where(f(a)*f(c)<0,c,b)\n",
    "        a = np.where(f(b)*f(c)<0,c,a)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bisectionNumpy2(-6,-4,1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bisectionNumpy2(1,3,1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 19.85 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000000 loops, best of 3: 565 ns per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "bisectionNumpy2(-6,-4,1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 矩阵实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bisectionArray(a,b):\n",
    "    eps = 3\n",
    "    n = 10**(eps*(abs(b-a)))\n",
    "    #n = 1000\n",
    "    x = np.linspace(a,b,n)\n",
    "    X = np.ones(n*3).reshape(n,3)\n",
    "    X[:,0] = (x*x)\n",
    "    X[:,1] = x\n",
    "    X[:,2] = np.ones((n,1))[:,0]\n",
    "    Theta = np.array([1,3,-10])\n",
    "    \n",
    "    y = X*Theta\n",
    "    ysum = y.sum(axis=1)\n",
    "    return X[np.where((ysum<10**-eps)&(ysum>n**-1))][0,1]\n",
    "    #X[np.where((ysum<0.1)&(ysum>0.00000001))][0,1]\n",
    "    \n",
    "# 这个方法有点傻，有点类似梯度法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0000010000010002"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bisectionArray(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.0001410001410003"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bisectionArray(-6,-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 71.8 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "bisectionArray(-6,-4)\n",
    "# 实际上没太多意义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Scipy实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.0, 2.0)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.bisect(f,-6,-4),opt.bisect(f,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.0000000000000036, 2.0000000000000036)"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.brentq(f,-6,-4),opt.brentq(f,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 11.58 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "100000 loops, best of 3: 1.81 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "opt.bisect(f,-6,-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 9.77 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "100000 loops, best of 3: 3.37 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "opt.brentq(f,-6,-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.总结\n",
    "\n",
    "1. 这篇关于[二分法](http://www.csie.ntnu.edu.tw/~u91029/RootFinding.html)的文章还不错。  \n",
    "2. 在[这个](http://www.zhihu.com/question/24789359)问题（在哪些方面，Numpy的速度反而比不上原始Python？）中，提到list和numpy速度快慢的问题。  \n",
    "3. 速度上来看，Numpy和Python方法比Scipy库要快。具体如下：  \n",
    "   * Python:1000000 loops, best of 3: 550 ns per loop  \n",
    "   * Numpy:1000000 loops, best of 3: 565 ns per loop  \n",
    "   * opt.bisect:100000 loops, best of 3: 1.81 µs per loop  \n",
    "   * opt.brentq:100000 loops, best of 3: 3.37 µs per loop\n",
    "4. 二分法是一种求解问题的思路和方法，也可以用来求极值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.什么是梯度下降法\n",
    "\n",
    "梯度下降法是一种常用的最优化方法，实现简单。一般情况下，其解不一定是全局最优解，速度也不是最快的。  \n",
    "优化思想：选取适当的初值，沿当前位置负梯度作为搜索方向，不断迭代，更新x的值，进行目标函数的极小化，直到收敛。  \n",
    "负梯度方向是使函数值下降最快的方向，越接近目标值，步长越小，前进越慢。\n",
    "\n",
    "**缺点**  \n",
    "（1）靠近极小值时收敛速度减慢。  \n",
    "（2）直线搜索时可能会产生一些问题。  \n",
    "（3）可能会“之”字形下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.梯度下降法步骤及算法  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：目标函数$f(x)$，梯度函数$g(x)=\\nabla f(x)$，计算精度$\\epsilon$；  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：$f(x)$的极小值点$x^*$。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）取初始值$x^0 \\in R^n$，置$k=0$  　　# k为迭代次数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）计算$f(x^k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（3）计算梯度$g_k = g(x^k)$，当$\\mid\\mid g_k \\mid\\mid < \\epsilon$时，停止迭代，令$x^* = x^k$；否则，令$p_k = -g(x^k)$，求$\\lambda_k$，使"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ f(x^k+\\lambda_kp_k) = min_{\\lambda\\geq0}(f(x^k+\\lambda_kp_k)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（4）置$x^{(k+1)}=x^k+\\lambda_kp_k$，计算$f(x^{k+1})$，当："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mid\\mid f(x^{k+1})-f(x^k)\\mid\\mid<\\epsilon$或$\\mid\\mid x^{k+1}-x^k\\mid\\mid<\\epsilon$时，停止迭代，令$x^*=x^{k+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（5）否则，置$k=k+1$，转（3）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.线性拟合中的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于线性回归，假设我们的估计方程为：$h_\\theta = \\theta_0 + \\theta_1x$，代价函数为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\theta_0,\\theta_1) = \\frac1{2m} \\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 批量梯度下降（BGD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量梯度算法公式为：\n",
    "\n",
    "repeat until convergence  {  \n",
    "  $\\theta_j: = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta_0,\\theta_1) \\quad\\quad \\text{for j = 0 and j=1，α表示学习率} $  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）将$J(\\theta_0,\\theta_1)$对$\\theta$求导，得到每个$\\theta$对应的梯度："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_1} = -\\frac1m \\sum_{i=1}^m((h_\\theta(x^{(i)})-y^{(i)})x_j^i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_0} = -\\frac1m \\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）算法变为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat {  \n",
    "$\\theta_0: = \\theta_0-\\alpha \\frac1m \\sum_{i=1}^m (h_\\theta(x^{(i)}-y^{(i)})$   \n",
    "$\\theta_1: = \\theta_1-\\alpha \\frac1m \\sum_{i=1}^m ((h_\\theta(x^{(i)})-y^{(i)})·x^{(i)})$   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（3）我们看出这样将会得到一个全局最优解，但是每迭代一步，都要计算到所有的训练集数据，如果训练数据集非常大（m很大），迭代速度会特别慢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 随机梯度下降（RGD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法如下：\n",
    "Repeat {  \n",
    "$\\theta_j: = \\theta_j-\\alpha ((h_\\theta(x^{(i)})-y^{(i)})·x_j^{(i)})$   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机梯度下降每次只用一个样本，虽然不是每次都向着全局最优解迭代，但整体方向是对的。  \n",
    "以损失精确度提高优化效率，适用于大规模训练集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.算法实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 标准python实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return x*2 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 标准python\n",
    "def gradient(x0,eps):\n",
    "    step = 0.01\n",
    "    x = x0\n",
    "    f_change = abs(f(x0))  \n",
    "    while (f_change > eps) | (abs(step*g(x)) > eps):\n",
    "        fx0 = f(x)\n",
    "        x = x - step*g(x)\n",
    "        f_change = abs(fx0 - f(x))\n",
    "    return f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.249999999975618"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(-100,0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.249999999975135"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(100,0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.249999999975746"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(0,0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 标准python2\n",
    "def gradient2(x0,step,eps):\n",
    "    x = x0\n",
    "    f_change = abs(f(x0))\n",
    "    if abs(g(x)) < eps:\n",
    "        return f(x)\n",
    "    else:\n",
    "        while (f_change > eps) | (abs(step*g(x)) > eps):\n",
    "            fx0 = f(x)\n",
    "            x = x - step*g(x)\n",
    "            f_change = abs(fx0 - f(x))\n",
    "        return f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.249999999999996"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient2(100,0.01,1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.249999999999998"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient2(-100,0.01,1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.249999999999998"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient2(0,0.01,1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 1.16 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit gradient(100,1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 1.14 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit gradient2(100,0.01,1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Numpy实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Numpy实现\n",
    "def gradientNumpy(x0,step,eps):\n",
    "    x = x0\n",
    "    f_change = abs(f(x0))\n",
    "    while (abs(step*g(x)) > eps):\n",
    "        x = x - step*g(x)\n",
    "        fx0 = f(x)\n",
    "        f_change = np.where(f_change > eps, abs(fx0 - f(x)), f_change)\n",
    "    return f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.249999999999996"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientNumpy(100,0.01,1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.249999999999998"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientNumpy(0,0.01,1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 3.51 ms per loop\n"
     ]
    }
   ],
   "source": [
    "% timeit gradientNumpy(100,0.01,1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Scipy库实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "from scipy.optimize import basinhopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -12.249999999999996\n",
       " hess_inv: array([[ 0.50000001]])\n",
       "      jac: array([ -1.19209290e-07])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 12\n",
       "      nit: 2\n",
       "     njev: 4\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([-1.50000006])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.minimize(f,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                        fun: -12.25\n",
       " lowest_optimization_result:       fun: -12.25\n",
       " hess_inv: array([[1]])\n",
       "      jac: array([  1.19209290e-07])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 9\n",
       "      nit: 1\n",
       "     njev: 3\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([-1.49999998])\n",
       "                    message: ['requested number of basinhopping iterations completed successfully']\n",
       "      minimization_failures: 0\n",
       "                       nfev: 999\n",
       "                        nit: 100\n",
       "                       njev: 333\n",
       "                          x: array([-1.49999998])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basinhopping(f,2,stepsize=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 213 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit opt.minimize(f,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 18.6 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit basinhopping(f,100,stepsize=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [这里](http://www.cnblogs.com/maybe2030/p/4751804.html)有关于几种算法的简要介绍。  \n",
    "2. 有关最小二乘法、梯度下降法的区别可以阅读[最小二乘法和梯度下降法有哪些区别](https://www.zhihu.com/question/20822481)和[线性回归、梯度下降、最小二乘的几何和概率解释](http://blog.csdn.net/luoshixian099/article/details/50880432)。  \n",
    "3. 速度上基本都差不多，opt.minimize速度最快，具体如下：  \n",
    "   *  Python:$\\quad$$\\quad$$\\quad$1000 loops, best of 3: 1.14 ms per loop  \n",
    "   *  Numpy:　　　 　100 loops, best of 3: 3.51 ms per loop  \n",
    "   *  opt.minimize:$\\quad$1000 loops, best of 3: 213  µs per loop  \n",
    "   *  bashihopping:$\\quad$10 loops, best of 3: 18.6 ms per loop  \n",
    "4. 梯度下降法是一种常用的最优化方法，也是体现了一种不断逼近最优值的路径。  \n",
    "5. 牛顿法和拟牛顿法[介绍](http://blog.csdn.net/itplus/article/details/21897443)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
